{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "purple-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/acui/torch_1.7.1_env/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import masked_networks\n",
    "from tf_plus import learning_phase, batchnorm_learning_phase\n",
    "from tf_plus import sess_run_dict, add_classification_losses\n",
    "from tf_plus import summarize_weights\n",
    "from train_supermask import make_parser, read_input_data, \\\n",
    "    init_model, load_initial_weights, split_and_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "three-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaparser = argparse.ArgumentParser()\n",
    "metaparser.add_argument('--experiment_name', type=str, required=True)\n",
    "metaparser.add_argument('--pretrained_epochs', type=int, required=True)\n",
    "meta_args = metaparser.parse_args()\n",
    "# meta_args = metaparser.parse_args([\"--experiment_name\", \"control_3\", \"--pretrained_epochs\", \"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fossil-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_dir(seed, meta_args):\n",
    "    attempt_num = 0\n",
    "    experiment_name = meta_args.experiment_name\n",
    "    pretrained_epochs = meta_args.pretrained_epochs\n",
    "\n",
    "    if experiment_name == \"control_3\":\n",
    "        input_dir = \"./results/iter_lot_fc_orig/learned_supermasks_seed_{seed}_attempt_{attempt_num}/run1\".format(\n",
    "            seed=seed,\n",
    "            attempt_num=attempt_num)\n",
    "\n",
    "    elif experiment_name == \"pretrained_supermask\":\n",
    "        input_dir = \"./results/iter_lot_fc_orig/learned_supermasks_pre_trained_{pretrained_epochs}_epochs_seed_{seed}_{attempt_num}/run1\".format(\n",
    "            pretrained_epochs=pretrained_epochs,\n",
    "            seed=seed,\n",
    "            attempt_num=attempt_num)\n",
    "    return input_dir\n",
    "\n",
    "\n",
    "parser = make_parser()\n",
    "# Have a seed just to satisfy the requirements\n",
    "seed = 1\n",
    "input_dir = build_input_dir(seed, meta_args)\n",
    "\n",
    "args_str = \"\"\"--train_h5 ./data/mnist_train.h5 --test_h5 ./data/mnist_test.h5\n",
    "--arch fc_mask --opt sgd --lr 100 --num_epochs 500 --print_every 220\n",
    "--eval_every 220 --log_every 220 --save_weights --save_every 22000\n",
    "--tf_seed {}\n",
    "--init_weights_h5 {}\n",
    "\"\"\".format(seed, input_dir).split()\n",
    "args = parser.parse_args(args_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enclosed-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing images by a factor of 255\n",
      "Data shapes: (55000, 28, 28, 1) (55000,) (10000, 28, 28, 1) (10000,)\n",
      "WARNING:tensorflow:From /home/acui/projects/deconstructing-lottery-tickets/masked_networks.py:183: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/acui/torch_1.7.1_env/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:90: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = read_input_data(args.train_h5)\n",
    "test_x, test_y = read_input_data(args.test_h5) # used as val for now\n",
    "images_scale = np.max(train_x)\n",
    "if images_scale > 1:\n",
    "    print('Normalizing images by a factor of {}'.format(images_scale))\n",
    "    train_x = train_x / images_scale\n",
    "    test_x = test_x / images_scale\n",
    "\n",
    "\n",
    "if args.test_batch_size == 0:\n",
    "    args.test_batch_size = test_y.shape[0]\n",
    "\n",
    "print('Data shapes:', train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "if train_y.shape[0] % args.train_batch_size != 0:\n",
    "    print(\"WARNING batch size doesn't divide train set evenly\")\n",
    "if train_y.shape[0] % args.large_batch_size != 0:\n",
    "    print(\"WARNING large batch size doesn't divide train set evenly\")\n",
    "if test_y.shape[0] % args.test_batch_size != 0:\n",
    "    print(\"WARNING batch size doesn't divide test set evenly\")\n",
    "\n",
    "# build model, masked networks\n",
    "if args.arch == 'fc_mask':\n",
    "    model = masked_networks.build_fc_supermask(args)\n",
    "else:\n",
    "    raise Exception(\"Not prepared for non fc_mask model\")\n",
    "        \n",
    "init_model(model, args)\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "south-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def visualize_mask_weights(mask_layers, seed):\n",
    "    num_bins = 20\n",
    "    for i, mask_layer in enumerate(mask_layers):\n",
    "        plt.subplot(len(mask_layers), 2, i * 2 + 1)\n",
    "        plt.hist(mask_layer.flatten(), bins=num_bins)\n",
    "        plt.xlabel(\"Raw mask values at layer {}\".format(i))\n",
    "        plt.subplot(len(mask_layers), 2, i * 2 + 2)\n",
    "        plt.hist(sigmoid(mask_layer.flatten()), bins=num_bins)\n",
    "        plt.xlabel(\"Sigmoided mask values at layer {}\".format(i))\n",
    "    plt.tight_layout()\n",
    "    if not os.path.exists('results/iter_lot_fc_orig/figs'):\n",
    "        os.mkdir('results/iter_lot_fc_orig/figs')\n",
    "    plt.savefig(os.path.join('results/iter_lot_fc_orig/figs',\n",
    "        \"mask_dists_epochs_{}_seed_{}.png\".format(meta_args.pretrained_epochs, seed)))\n",
    "    \n",
    "def get_test_accs(run_dir):\n",
    "    test_accs = []\n",
    "    for filename in os.listdir(run_dir):\n",
    "        if 'tfevents' in filename:\n",
    "            for e in tf.compat.v1.train.summary_iterator(os.path.join(\n",
    "                run_dir, filename\n",
    "            )):\n",
    "                for v in e.summary.value:\n",
    "                    if v.tag == 'eval_test_acc':\n",
    "                        test_accs.append(v.simple_value)\n",
    "    return np.array(test_accs)\n",
    "\n",
    "def ccdf(data):\n",
    "    s_data = np.sort(data)[::-1]\n",
    "    return np.stack([s_data, np.arange(s_data.shape[0]) + 1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "peripheral-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_on_seed(seed, meta_args):\n",
    "    seed_info = {\n",
    "        \"seed\": seed,\n",
    "        \"regular_epochs\": meta_args.pretrained_epochs,\n",
    "        \"supermask_epochs\": 500,\n",
    "        \"experiment_name\": meta_args.experiment_name,\n",
    "        \"has_supermask\": 1,\n",
    "        \"has_lth\": 0,\n",
    "        \"test_accuracy\": None\n",
    "    }\n",
    "    \n",
    "    args.init_weights_h5 = build_input_dir(seed, meta_args)\n",
    "    if not args.init_weights_h5.endswith('/weights'):\n",
    "        h5file = os.path.join(args.init_weights_h5, 'weights')\n",
    "    else:\n",
    "        h5file = args.init_weights_h5\n",
    "    hf_weights = h5py.File(h5file, 'r')\n",
    "    all_weights = hf_weights.get('all_weights')\n",
    "    print(\"Number of weight copies:\", len(all_weights))\n",
    "    init_weights_flat = all_weights[0]\n",
    "    final_weights_flat = all_weights[-1]\n",
    "    current_mask = np.array(hf_weights.get('mask_values'))\n",
    "\n",
    "    shapes = [literal_eval(s) for s in hf_weights.attrs['var_shapes'].decode('utf-8').split(';')]\n",
    "    hf_weights.close()\n",
    "\n",
    "    weight_values = split_and_shape(init_weights_flat, shapes)\n",
    "    final_weight_values = split_and_shape(final_weights_flat, shapes)\n",
    "    gk = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    if len(gk) > 9:\n",
    "        print(\"You need to restart the kernel - graphkeys have been replicated\" +\n",
    "              \" and there's no going back\")\n",
    "\n",
    "    for i, w in enumerate(gk):\n",
    "       #if 'mask' not in w.name: # HACK for biased masks\n",
    "        print('loading weights for layer {}: {}'.format(i, w.name))\n",
    "        w.load(weight_values[i], session=sess)\n",
    "\n",
    "    # Mask analysis    \n",
    "    \n",
    "    mask_layers = final_weight_values[2::3]\n",
    "    print(\"Basic info\")\n",
    "    all_mask_weights = []\n",
    "    for mask_layer in mask_layers:\n",
    "        print(\"Shape:\", mask_layer.shape)\n",
    "        print(\"Average fraction masked:\", 1-sigmoid(mask_layer).mean())\n",
    "        print(\"Min: {}, Max: {}\\n\".format(\n",
    "            mask_layer.min(), mask_layer.max()))\n",
    "        all_mask_weights.append(mask_layer.flatten())\n",
    "    all_mask_weights = np.concatenate(all_mask_weights)\n",
    "    \n",
    "    seed_info[\"fraction_supermasked\"] = 1-sigmoid(all_mask_weights).mean()\n",
    "    print(\"Total average fraction masked:\", seed_info[\"fraction_supermasked\"])\n",
    "    \n",
    "    # Number of bernoulli samples. Not much difference between samples observed\n",
    "    n_bern = 3\n",
    "    sampled_infos = []\n",
    "\n",
    "    for i in range(n_bern):\n",
    "        DG = nx.DiGraph()\n",
    "        edges = []\n",
    "        nodes = set([\"s\", \"t\"])\n",
    "        masked_weights = []\n",
    "        sampled_info = {}\n",
    "        \n",
    "        for j, mask_layer in enumerate(mask_layers):\n",
    "            clamped_mask = np.random.binomial(1, sigmoid(mask_layer))\n",
    "            masked_weights.append(clamped_mask * final_weight_values[j * 3])\n",
    "            \n",
    "            # Don't do graph processing for first layer, too many edges\n",
    "            if j == 0:\n",
    "                continue\n",
    "            for row_ind, row in enumerate(clamped_mask):\n",
    "                for col_ind, val in enumerate(row):\n",
    "                    in_node = \"{}_{}\".format(j, row_ind)\n",
    "                    out_node = \"{}_{}\".format(j + 1, col_ind)\n",
    "                    nodes.add(in_node)\n",
    "                    nodes.add(out_node)\n",
    "                    \n",
    "                    if j == 1:\n",
    "                        edges.append([\"s\", in_node])\n",
    "                    elif j == len(mask_layers) - 1:\n",
    "                        edges.append([out_node, \"t\"])\n",
    "                    \n",
    "                    if val == 1:\n",
    "                        edge = [in_node, out_node]\n",
    "                        DG.add_edge(*edge, capacity=abs(val))\n",
    "        DG.add_nodes_from(nodes)\n",
    "        DG.add_edges_from(edges, weight=1, capacity=10e10)\n",
    "        all_masked_weights = np.concatenate([\n",
    "            masked_weight.flatten() for masked_weight in masked_weights\n",
    "        ])\n",
    "        # Just get weight matrices\n",
    "        all_init_weights = np.concatenate([\n",
    "            init_weight.flatten() for init_weight in weight_values[::3]\n",
    "        ])\n",
    "        \n",
    "        out_degree_dist = np.array([pair[1] for pair in DG.out_degree()])\n",
    "        in_degree_dist = np.array([pair[1] for pair in DG.in_degree()])\n",
    "        out_deg_ccdf = ccdf(out_degree_dist)\n",
    "        in_deg_ccdf = ccdf(in_degree_dist)\n",
    "        if i == 0:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(in_deg_ccdf[:, 1], in_deg_ccdf[:, 0], label='In degree')\n",
    "            plt.scatter(out_deg_ccdf[:, 1], out_deg_ccdf[:, 0], label='Out degree')\n",
    "            plt.xlabel(\"Rank\")\n",
    "            plt.ylabel(\"Degree rank CCDF\")\n",
    "            plt.legend()\n",
    "    #         plt.show()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(in_degree_dist, out_degree_dist)\n",
    "            plt.xlabel(\"In degree dist\")\n",
    "            plt.ylabel(\"Out degree dist\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join('results/iter_lot_fc_orig/figs',\n",
    "                \"degree_dists_epochs_{}_seed_{}.png\".format(meta_args.pretrained_epochs, seed)))\n",
    "            plt.show()\n",
    "\n",
    "        # Properties\n",
    "        G = nx.Graph(DG)\n",
    "        sampled_info[\"out_degree_mean\"] = out_degree_dist.mean()\n",
    "        sampled_info[\"out_degree_std\"] = out_degree_dist.std()\n",
    "#         print(\"Mean and std of out degree:\", out_degree_dist.mean(),\n",
    "#                out_degree_dist.std())\n",
    "#         print(\"Average diameter:\", nx.average_shortest_path_length(G)) # not meaningful\n",
    "#         print(\"Approx Connectivity:\", nx.node_connectivity(G)) # usually 40-43\n",
    "        sampled_info[\"maximum_flow\"] = nx.maximum_flow(G, \"s\", \"t\")[0]\n",
    "#         print(\"Maximum flow:\", nx.maximum_flow(G, \"s\", \"t\")[0])\n",
    "\n",
    "        sampled_info[\"weights_mean\"] = all_mask_weights.mean()\n",
    "        sampled_info[\"weights_std\"] = all_mask_weights.std()\n",
    "#         print(\"Weights variance, absolute mean\", all_mask_weights.std(),\n",
    "#               np.absolute(all_mask_weights).mean())\n",
    "        \n",
    "        # Best generalization bounds\n",
    "        d = len(masked_weights)  # number of layers\n",
    "        m = train_y.shape[0]  # length of the dataset\n",
    "        # pacbayes.orig, pacbayes.flatness, pacbayes.init\n",
    "        sigma = 1  # TODO: Not correct - need to calculate it\n",
    "        all_weights_dist = all_masked_weights - all_init_weights\n",
    "        def pacbayes_bound(vec):\n",
    "            return np.power(np.linalg.norm(vec), 2) / (4 * sigma ** 2) + \\\n",
    "                np.log(m / sigma) + 10\n",
    "        sampled_info[\"pacbayes.orig\"] = pacbayes_bound(all_masked_weights)\n",
    "        sampled_info[\"pacbayes.init\"] = pacbayes_bound(all_weights_dist)\n",
    "        sampled_info[\"pacbayes.flatness\"] = 1 / sigma ** 2\n",
    "#         print(\"pacbayes.orig\", pacbayes_bound(all_masked_weights))\n",
    "#         print(\"pacbayes.init\", pacbayes_bound(all_weights_dist))\n",
    "#         print(\"pacbayes.flatness\", 1 / sigma ** 2)\n",
    "        # path.norm.over.margin\n",
    "        # path.norm\n",
    "        # fro.dist\n",
    "        dist_frob_norm = np.linalg.norm(all_weights_dist)\n",
    "        sampled_info[\"fro.dist\"] = dist_frob_norm.sum()\n",
    "#         print(\"fro.dist\", dist_frob_norm.sum())\n",
    "        # sum.of.fro\n",
    "        frob_norm = np.linalg.norm(all_masked_weights)\n",
    "        sampled_info[\"log.sum.of.fro\"] = np.log(frob_norm).sum() / d + np.log(d)\n",
    "#         print(\"log.sum.of.fro\", np.log(frob_norm).sum() / d + np.log(d))\n",
    "        sampled_infos.append(sampled_info)\n",
    "    for sampled_info in sampled_infos:\n",
    "        for key in sampled_info:\n",
    "            if key not in seed_info:\n",
    "                seed_info[key] = sampled_info[key] / n_bern\n",
    "            else:\n",
    "                seed_info[key] += sampled_info[key] / n_bern\n",
    "    # Visualize and save img of mask weight distribution\n",
    "    visualize_mask_weights(mask_layers, seed)\n",
    "    \n",
    "    seed_info[\"test_accuracy\"] = get_test_accs(args.init_weights_h5).max()\n",
    "    for key in seed_info:\n",
    "        print(key, seed_info[key])\n",
    "    return seed_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerical-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.49366632629611207\n",
      "Min: -8.745142936706543, Max: 8.770081520080566\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.4430654096143837\n",
      "Min: -9.099223136901855, Max: 9.445528984069824\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2941315581819023\n",
      "Min: -11.061659812927246, Max: 11.945895195007324\n",
      "\n",
      "Total average fraction masked: 0.48721417652689325\n",
      "seed 1\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9483000040054321\n",
      "fraction_supermasked 0.48721417652689325\n",
      "out_degree_mean 43.03074433656958\n",
      "out_degree_std 25.63039144781579\n",
      "maximum_flow 706.6666666666667\n",
      "weights_mean 0.18344981766025714\n",
      "weights_std 4.195476468438395\n",
      "pacbayes.orig 102.64328393626714\n",
      "pacbayes.init 90.06804316397448\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.631646496368226\n",
      "log.sum.of.fro 2.0635610583566963\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.49401024543782923\n",
      "Min: -8.639673233032227, Max: 8.84908390045166\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.44542809403215067\n",
      "Min: -9.390898704528809, Max: 9.471814155578613\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.266864942915776\n",
      "Min: -11.189888000488281, Max: 12.006192207336426\n",
      "\n",
      "Total average fraction masked: 0.48768188388751965\n",
      "seed 2\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9506999850273132\n",
      "fraction_supermasked 0.48768188388751965\n",
      "out_degree_mean 42.90776699029126\n",
      "out_degree_std 25.43702931956462\n",
      "maximum_flow 733.0\n",
      "weights_mean 0.1802562526350808\n",
      "weights_std 4.199884896188153\n",
      "pacbayes.orig 102.44049638542046\n",
      "pacbayes.init 89.96444600154476\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.619186904862577\n",
      "log.sum.of.fro 2.0631470872239044\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.49527027007267577\n",
      "Min: -8.856532096862793, Max: 8.800651550292969\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.4476040438964364\n",
      "Min: -9.175764083862305, Max: 9.510625839233398\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2679447130570237\n",
      "Min: -10.46463680267334, Max: 11.938383102416992\n",
      "\n",
      "Total average fraction masked: 0.48904445361023086\n",
      "seed 3\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9496999979019165\n",
      "fraction_supermasked 0.48904445361023086\n",
      "out_degree_mean 42.73705501618123\n",
      "out_degree_std 25.34464077121848\n",
      "maximum_flow 731.0\n",
      "weights_mean 0.16551917328808735\n",
      "weights_std 4.195293613477463\n",
      "pacbayes.orig 101.81614459267021\n",
      "pacbayes.init 89.70070102885113\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.587407528269637\n",
      "log.sum.of.fro 2.061865511270176\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.4936519118269175\n",
      "Min: -8.797584533691406, Max: 8.775428771972656\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.4369212863786093\n",
      "Min: -9.463261604309082, Max: 9.599008560180664\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.28840648941474367\n",
      "Min: -10.822389602661133, Max: 11.954116821289062\n",
      "\n",
      "Total average fraction masked: 0.48648750842398203\n",
      "seed 4\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.949400007724762\n",
      "fraction_supermasked 0.48648750842398203\n",
      "out_degree_mean 43.48705501618123\n",
      "out_degree_std 25.82215625759416\n",
      "maximum_flow 712.0\n",
      "weights_mean 0.19300148592989025\n",
      "weights_std 4.206619485306745\n",
      "pacbayes.orig 102.73671748919068\n",
      "pacbayes.init 89.68845714055921\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.58593944823837\n",
      "log.sum.of.fro 2.063751544523776\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.49377564732348866\n",
      "Min: -8.79906177520752, Max: 8.777606010437012\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.4387572350564827\n",
      "Min: -9.106077194213867, Max: 9.39047622680664\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2701722657263711\n",
      "Min: -11.514673233032227, Max: 11.890459060668945\n",
      "\n",
      "Total average fraction masked: 0.48673524255411493\n",
      "seed 5\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9502000212669373\n",
      "fraction_supermasked 0.48673524255411493\n",
      "out_degree_mean 43.396440129449836\n",
      "out_degree_std 25.685165515360644\n",
      "maximum_flow 729.6666666666667\n",
      "weights_mean 0.18569961099381624\n",
      "weights_std 4.177733389655696\n",
      "pacbayes.orig 102.50165045281713\n",
      "pacbayes.init 89.3109662106014\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.54035943003148\n",
      "log.sum.of.fro 2.063272092093294\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.4947505609114091\n",
      "Min: -8.63525676727295, Max: 8.78791332244873\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.445423152955311\n",
      "Min: -9.14242935180664, Max: 9.405923843383789\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2712245623994629\n",
      "Min: -10.712346076965332, Max: 11.890719413757324\n",
      "\n",
      "Total average fraction masked: 0.48835180720293836\n",
      "seed 6\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9498000144958496\n",
      "fraction_supermasked 0.48835180720293836\n",
      "out_degree_mean 42.89886731391586\n",
      "out_degree_std 25.408823052025156\n",
      "maximum_flow 728.6666666666666\n",
      "weights_mean 0.1716669686402534\n",
      "weights_std 4.179230039272893\n",
      "pacbayes.orig 102.8088609461804\n",
      "pacbayes.init 90.46604686655722\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.679441264230732\n",
      "log.sum.of.fro 2.063898444274813\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.4954210737768062\n",
      "Min: -9.154684066772461, Max: 8.8885498046875\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.44294386145563036\n",
      "Min: -9.34191608428955, Max: 9.41357135772705\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2607077165066547\n",
      "Min: -10.82752513885498, Max: 12.085275650024414\n",
      "\n",
      "Total average fraction masked: 0.4886253197313313\n",
      "seed 7\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9470000267028809\n",
      "fraction_supermasked 0.4886253197313313\n",
      "out_degree_mean 43.11812297734628\n",
      "out_degree_std 25.49387123177717\n",
      "maximum_flow 739.0\n",
      "weights_mean 0.16834448995438323\n",
      "weights_std 4.2101290824656505\n",
      "pacbayes.orig 102.55871974807866\n",
      "pacbayes.init 90.26061588698093\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.654790643886404\n",
      "log.sum.of.fro 2.063388600988009\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.4923796902370864\n",
      "Min: -9.009828567504883, Max: 8.76710319519043\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.44213113848778895\n",
      "Min: -9.375802040100098, Max: 9.398818016052246\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.30031300841094366\n",
      "Min: -10.73093318939209, Max: 12.092970848083496\n",
      "\n",
      "Total average fraction masked: 0.4859953054350389\n",
      "seed 8\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9472000002861023\n",
      "fraction_supermasked 0.4859953054350389\n",
      "out_degree_mean 43.08171521035599\n",
      "out_degree_std 25.62552433911902\n",
      "maximum_flow 701.6666666666666\n",
      "weights_mean 0.1965648191822346\n",
      "weights_std 4.217834174261513\n",
      "pacbayes.orig 102.59543022747448\n",
      "pacbayes.init 89.54981388243606\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.569215357255366\n",
      "log.sum.of.fro 2.0634635716615137\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.49346783716346987\n",
      "Min: -8.721031188964844, Max: 8.959908485412598\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.4388893636481721\n",
      "Min: -9.227861404418945, Max: 9.400235176086426\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2735988413933853\n",
      "Min: -10.69752025604248, Max: 11.974520683288574\n",
      "\n",
      "Total average fraction masked: 0.48649104076516403\n",
      "seed 9\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9480000138282776\n",
      "fraction_supermasked 0.48649104076516403\n",
      "out_degree_mean 43.34546925566343\n",
      "out_degree_std 25.653448982214023\n",
      "maximum_flow 726.6666666666667\n",
      "weights_mean 0.19024853322479884\n",
      "weights_std 4.195157545955153\n",
      "pacbayes.orig 102.57190820383761\n",
      "pacbayes.init 89.80614764946\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.60012730715969\n",
      "log.sum.of.fro 2.063415563468332\n",
      "Number of weight copies: 6\n",
      "loading weights for layer 0: sequential_network/fc_1/kernel:0\n",
      "loading weights for layer 1: sequential_network/fc_1/bias:0\n",
      "loading weights for layer 2: sequential_network/fc_1/mask:0\n",
      "loading weights for layer 3: sequential_network/fc_2/kernel:0\n",
      "loading weights for layer 4: sequential_network/fc_2/bias:0\n",
      "loading weights for layer 5: sequential_network/fc_2/mask:0\n",
      "loading weights for layer 6: sequential_network/fc_3/kernel:0\n",
      "loading weights for layer 7: sequential_network/fc_3/bias:0\n",
      "loading weights for layer 8: sequential_network/fc_3/mask:0\n",
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.49410051468580796\n",
      "Min: -8.559091567993164, Max: 8.712869644165039\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.44136655289678917\n",
      "Min: -9.153130531311035, Max: 9.33031177520752\n",
      "\n",
      "Shape: (100, 10)\n",
      "Average fraction masked: 0.2648837779309937\n",
      "Min: -10.517205238342285, Max: 11.761845588684082\n",
      "\n",
      "Total average fraction masked: 0.48729647415077637\n",
      "seed 10\n",
      "regular_epochs 0\n",
      "supermask_epochs 500\n",
      "experiment_name control_3\n",
      "has_supermask 1\n",
      "has_lth 0\n",
      "test_accuracy 0.9495999813079834\n",
      "fraction_supermasked 0.48729647415077637\n",
      "out_degree_mean 43.17556634304207\n",
      "out_degree_std 25.53468277128072\n",
      "maximum_flow 734.3333333333334\n",
      "weights_mean 0.18124874568976698\n",
      "weights_std 4.177437919812699\n",
      "pacbayes.orig 102.82485555302623\n",
      "pacbayes.init 89.78095182385877\n",
      "pacbayes.flatness 1.0\n",
      "fro.dist 16.597086693894166\n",
      "log.sum.of.fro 2.0639309127577885\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_infos = []\n",
    "\n",
    "for seed in range(1, 11):\n",
    "    seed_info = run_analysis_on_seed(seed, meta_args)\n",
    "    seed_infos.append(seed_info)\n",
    "    \n",
    "df = pd.DataFrame(seed_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "selective-robin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>regular_epochs</th>\n",
       "      <th>supermask_epochs</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>has_supermask</th>\n",
       "      <th>has_lth</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>fraction_supermasked</th>\n",
       "      <th>out_degree_mean</th>\n",
       "      <th>out_degree_std</th>\n",
       "      <th>maximum_flow</th>\n",
       "      <th>weights_mean</th>\n",
       "      <th>weights_std</th>\n",
       "      <th>pacbayes.orig</th>\n",
       "      <th>pacbayes.init</th>\n",
       "      <th>pacbayes.flatness</th>\n",
       "      <th>fro.dist</th>\n",
       "      <th>log.sum.of.fro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.487214</td>\n",
       "      <td>43.030744</td>\n",
       "      <td>25.630391</td>\n",
       "      <td>706.666667</td>\n",
       "      <td>0.183450</td>\n",
       "      <td>4.195476</td>\n",
       "      <td>102.643284</td>\n",
       "      <td>90.068043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.631646</td>\n",
       "      <td>2.063561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9507</td>\n",
       "      <td>0.487682</td>\n",
       "      <td>42.907767</td>\n",
       "      <td>25.437029</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>0.180256</td>\n",
       "      <td>4.199885</td>\n",
       "      <td>102.440496</td>\n",
       "      <td>89.964446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.619187</td>\n",
       "      <td>2.063147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>0.489044</td>\n",
       "      <td>42.737055</td>\n",
       "      <td>25.344641</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>0.165519</td>\n",
       "      <td>4.195294</td>\n",
       "      <td>101.816145</td>\n",
       "      <td>89.700701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.587408</td>\n",
       "      <td>2.061866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>0.486488</td>\n",
       "      <td>43.487055</td>\n",
       "      <td>25.822156</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>0.193001</td>\n",
       "      <td>4.206619</td>\n",
       "      <td>102.736717</td>\n",
       "      <td>89.688457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.585939</td>\n",
       "      <td>2.063752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>0.486735</td>\n",
       "      <td>43.396440</td>\n",
       "      <td>25.685166</td>\n",
       "      <td>729.666667</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>4.177733</td>\n",
       "      <td>102.501650</td>\n",
       "      <td>89.310966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.540359</td>\n",
       "      <td>2.063272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9498</td>\n",
       "      <td>0.488352</td>\n",
       "      <td>42.898867</td>\n",
       "      <td>25.408823</td>\n",
       "      <td>728.666667</td>\n",
       "      <td>0.171667</td>\n",
       "      <td>4.179230</td>\n",
       "      <td>102.808861</td>\n",
       "      <td>90.466047</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.679441</td>\n",
       "      <td>2.063898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.488625</td>\n",
       "      <td>43.118123</td>\n",
       "      <td>25.493871</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>0.168344</td>\n",
       "      <td>4.210129</td>\n",
       "      <td>102.558720</td>\n",
       "      <td>90.260616</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.654791</td>\n",
       "      <td>2.063389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>0.485995</td>\n",
       "      <td>43.081715</td>\n",
       "      <td>25.625524</td>\n",
       "      <td>701.666667</td>\n",
       "      <td>0.196565</td>\n",
       "      <td>4.217834</td>\n",
       "      <td>102.595430</td>\n",
       "      <td>89.549814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.569215</td>\n",
       "      <td>2.063464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.486491</td>\n",
       "      <td>43.345469</td>\n",
       "      <td>25.653449</td>\n",
       "      <td>726.666667</td>\n",
       "      <td>0.190249</td>\n",
       "      <td>4.195158</td>\n",
       "      <td>102.571908</td>\n",
       "      <td>89.806148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.600127</td>\n",
       "      <td>2.063416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>control_3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.487296</td>\n",
       "      <td>43.175566</td>\n",
       "      <td>25.534683</td>\n",
       "      <td>734.333333</td>\n",
       "      <td>0.181249</td>\n",
       "      <td>4.177438</td>\n",
       "      <td>102.824856</td>\n",
       "      <td>89.780952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.597087</td>\n",
       "      <td>2.063931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  regular_epochs  supermask_epochs experiment_name  has_supermask  \\\n",
       "0     1               0               500       control_3              1   \n",
       "1     2               0               500       control_3              1   \n",
       "2     3               0               500       control_3              1   \n",
       "3     4               0               500       control_3              1   \n",
       "4     5               0               500       control_3              1   \n",
       "5     6               0               500       control_3              1   \n",
       "6     7               0               500       control_3              1   \n",
       "7     8               0               500       control_3              1   \n",
       "8     9               0               500       control_3              1   \n",
       "9    10               0               500       control_3              1   \n",
       "\n",
       "   has_lth  test_accuracy  fraction_supermasked  out_degree_mean  \\\n",
       "0        0         0.9483              0.487214        43.030744   \n",
       "1        0         0.9507              0.487682        42.907767   \n",
       "2        0         0.9497              0.489044        42.737055   \n",
       "3        0         0.9494              0.486488        43.487055   \n",
       "4        0         0.9502              0.486735        43.396440   \n",
       "5        0         0.9498              0.488352        42.898867   \n",
       "6        0         0.9470              0.488625        43.118123   \n",
       "7        0         0.9472              0.485995        43.081715   \n",
       "8        0         0.9480              0.486491        43.345469   \n",
       "9        0         0.9496              0.487296        43.175566   \n",
       "\n",
       "   out_degree_std  maximum_flow  weights_mean  weights_std  pacbayes.orig  \\\n",
       "0       25.630391    706.666667      0.183450     4.195476     102.643284   \n",
       "1       25.437029    733.000000      0.180256     4.199885     102.440496   \n",
       "2       25.344641    731.000000      0.165519     4.195294     101.816145   \n",
       "3       25.822156    712.000000      0.193001     4.206619     102.736717   \n",
       "4       25.685166    729.666667      0.185700     4.177733     102.501650   \n",
       "5       25.408823    728.666667      0.171667     4.179230     102.808861   \n",
       "6       25.493871    739.000000      0.168344     4.210129     102.558720   \n",
       "7       25.625524    701.666667      0.196565     4.217834     102.595430   \n",
       "8       25.653449    726.666667      0.190249     4.195158     102.571908   \n",
       "9       25.534683    734.333333      0.181249     4.177438     102.824856   \n",
       "\n",
       "   pacbayes.init  pacbayes.flatness   fro.dist  log.sum.of.fro  \n",
       "0      90.068043                1.0  16.631646        2.063561  \n",
       "1      89.964446                1.0  16.619187        2.063147  \n",
       "2      89.700701                1.0  16.587408        2.061866  \n",
       "3      89.688457                1.0  16.585939        2.063752  \n",
       "4      89.310966                1.0  16.540359        2.063272  \n",
       "5      90.466047                1.0  16.679441        2.063898  \n",
       "6      90.260616                1.0  16.654791        2.063389  \n",
       "7      89.549814                1.0  16.569215        2.063464  \n",
       "8      89.806148                1.0  16.600127        2.063416  \n",
       "9      89.780952                1.0  16.597087        2.063931  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "communist-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/iter_lot_fc_orig/results_summary_{}_{}.csv'.format(\n",
    "    meta_args.experiment_name, meta_args.pretrained_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-revision",
   "metadata": {},
   "source": [
    "# Analyzing the percentile-based pruning mask\n",
    "\n",
    "This code looks at the mask you get when you filter the weights by percentile for the purpose of lottery-ticket hypothesis system pruning and retraining.\n",
    "\n",
    "You should NOT run this with the supermask models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "important-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.method = 'large_final_same_sign'\n",
    "args.prev_weights = args.init_weights_h5\n",
    "args.output_dir = args.init_weights_h5\n",
    "assert('weights' not in args.output_dir)\n",
    "args.prune_base = \"0.8,0.9\"\n",
    "args.prune_power = 1\n",
    "args.layer_cutoff = \"4,6\"\n",
    "args.seed = args.tf_seed\n",
    "args.final_weight_interpolation = 1\n",
    "args.final_weights_ind = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "chicken-spine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results/iter_lot_fc_orig/learned_supermasks_seed_1_attempt_0/run1'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.init_weights_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "identified-melissa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python get_weight_init.py --method large_final_same_sign --weights_h5 ./results/iter_lot_fc_orig/learned_supermasks_seed_1_attempt_0/run1/weights --output_h5 ./results/iter_lot_fc_orig/learned_supermasks_seed_1_attempt_0/run1/init_weights --prune_base 0.8,0.9 --prune_power 1 --layer_cutoff 4,6 --seed 1 --final_weight_interpolation 1 --final_weights_ind -1'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_cmd = (\n",
    "    'python get_weight_init.py --method ' + args.method +\n",
    "    ' --weights_h5 ' + args.prev_weights + '/weights' +\n",
    "    ' --output_h5 ' + args.output_dir + '/init_weights' +\n",
    "    ' --prune_base ' + args.prune_base +\n",
    "    ' --prune_power ' + str(args.prune_power) +\n",
    "    ' --layer_cutoff ' + args.layer_cutoff + \n",
    "    ' --seed ' + str(args.seed) +\n",
    "    ' --final_weight_interpolation ' + str(args.final_weight_interpolation) +\n",
    "    ' --final_weights_ind ' + str(args.final_weights_ind))\n",
    "# os.system(preproc_cmd)\n",
    "preproc_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "preceding-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "proc_out = subprocess.run(preproc_cmd.split(), capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "liked-retirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==prune percentiles are: \n",
      "[19.999999999999996, 0, 19.999999999999996, 0, 9.999999999999998, 0]\n",
      "==no mask exists - generating first mask\n",
      "\n",
      "2021-06-01 02:27:18.675506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "get_weight_init.py:47: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  prune_num = np.int(np.sum(cur_mask) * percentile / 100)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(proc_out.stdout.decode('utf-8'))\n",
    "print(proc_out.stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "wooden-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.init_weights_h5.endswith('/init_weights'):\n",
    "    preproc_h5file = os.path.join(args.init_weights_h5, 'init_weights')\n",
    "else:\n",
    "    preproc_h5file = args.init_weights_h5\n",
    "preproc_hf_weights = h5py.File(preproc_h5file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "laughing-constraint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532810,)\n",
      "(532810,)\n"
     ]
    }
   ],
   "source": [
    "mask_flat = np.array(preproc_hf_weights.get('mask_values'))\n",
    "print(mask_flat.shape)\n",
    "print(init_weights_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "artificial-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_values = split_and_shape(mask_flat, shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "solved-eligibility",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "this code should NOT be used with supermask",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-2ce6c0f3b070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this code should NOT be used with supermask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: this code should NOT be used with supermask"
     ]
    }
   ],
   "source": [
    "if len(mask_values) == 9:\n",
    "    raise Exception(\"this code should NOT be used with supermask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "inner-monitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic info\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.19999574829931976\n",
      "Min: 0.0, Max: 1.0\n",
      "\n",
      "Shape: (300,)\n",
      "Average fraction masked: 0.0\n",
      "Min: 1.0, Max: 1.0\n",
      "\n",
      "Shape: (784, 300)\n",
      "Average fraction masked: 0.19999574829931976\n",
      "Min: 0.0, Max: 1.0\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.0\n",
      "Min: 1.0, Max: 1.0\n",
      "\n",
      "Shape: (100,)\n",
      "Average fraction masked: 0.08999999999999997\n",
      "Min: 0.0, Max: 1.0\n",
      "\n",
      "Shape: (300, 100)\n",
      "Average fraction masked: 0.0\n",
      "Min: 1.0, Max: 1.0\n",
      "\n",
      "Total average fraction masked: 0.1772550866616428\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic info\")\n",
    "all_mask_weights = []\n",
    "for mask_layer in mask_values:\n",
    "    print(\"Shape:\", mask_layer.shape)\n",
    "    print(\"Average fraction masked:\", 1 - mask_layer.mean())\n",
    "    print(\"Min: {}, Max: {}\\n\".format(\n",
    "        mask_layer.min(), mask_layer.max()))\n",
    "    all_mask_weights.append(mask_layer.flatten())\n",
    "all_mask_weights = np.concatenate(all_mask_weights)\n",
    "print(\"Total average fraction masked:\", 1-all_mask_weights.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
